import requests 
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
import time
import pandas as pd

driver = webdriver.Chrome()
driver.get('https://www.imdb.com/title/tt6751668/reviews/?ref_=tt_ql_urv')
time.sleep(3)

for k in range(99999):
    try:
        driver.find_element(by=By.ID, value="load-more-trigger").click()
        time.sleep(2)
    except Exception as ex:
        
        break;

html = driver.page_source
soup = BeautifulSoup(html, "lxml")

title_list = []
rating_list = []
review_list = []
review_date_list = []

box_list= soup.select("div.lister > div.lister-list > div")
for box in box_list:
    try:
        rating= box.select_one('span.rating-other-user-rating > span').text
    except:
        rating= "no rating"
    title= box.select_one('div.review-container > div.lister-item-content > a').text
    review= box.select_one('div.review-container > div.lister-item-content > div.content > div').text
    review_date= box.select_one('div.review-container > div.lister-item-content > div.display-name-date > span.review-date').text
    
    review_list.append(review)
    rating_list.append(rating)
    title_list.append(title)
    review_date_list.append(review_date)

info = {"rating" : rating_list , "title": title_list , "review" : review_list, "review_date" : review_date_list}   

parasite = pd.DataFrame(info)
parasite.to_csv("parasite_imdb.csv", encoding = "utf8", index=False)
